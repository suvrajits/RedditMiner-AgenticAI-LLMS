{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28974e9f-31f7-404e-9937-57ffcad3c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: pandas in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suvra\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install praw psycopg2 pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b26658-3f19-4188-b86e-3391720f67c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3868366585.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 37\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"game dev\", \"game programmer\", \"indie game\", \"game project\", \"hiring programmer\",\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries if not installed\n",
    "\n",
    "\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import difflib\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from IPython.display import display  # Standard Jupyter display\n",
    "\n",
    "# ✅ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Initialize Reddit API Client\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "        client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "        user_agent=os.getenv(\"REDDIT_USER_AGENT\"),\n",
    "        redirect_uri=os.getenv(\"REDDIT_REDIRECT_URI\")\n",
    "    )\n",
    "    \n",
    "    # ✅ Test API Connection\n",
    "    print(f\"✅ Authenticated as: {reddit.user.me()}\")  # Should print None (app-based auth)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Reddit API Authentication Failed: {e}\")\n",
    "\n",
    "# ✅ Define Relevant Subreddits\n",
    "SUBREDDITS = [\"gamedev\", \"forhire\", \"gameDevClassifieds\", \"INAT\"]\n",
    "\n",
    "# ✅ Define Expanded Keywords for Lead Generation\n",
    "KEYWORDS = [\"game developer\", \"hiring game\", \"unity developer\", \"unreal developer\", \n",
    "            \"game dev\", \"game programmer\", \"indie game\", \"game project\", \"hiring programmer\",\n",
    "            \"need game coder\", \"looking for dev\", \"develop a game\", \"game artists\", \"game hiring\"]\n",
    "\n",
    "# ✅ Email Extraction Function\n",
    "def extract_email(text):\n",
    "    \"\"\"Extracts the first email found in the text, if any.\"\"\"\n",
    "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    return emails[0] if emails else \"N/A\"\n",
    "\n",
    "# ✅ Fuzzy Keyword Matching\n",
    "def fuzzy_match(text, keywords, threshold=0.6):\n",
    "    \"\"\"Performs fuzzy matching for keyword relevance.\"\"\"\n",
    "    text_words = text.split()\n",
    "    for word in text_words:\n",
    "        matches = difflib.get_close_matches(word.lower(), keywords, n=1, cutoff=threshold)\n",
    "        if matches:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ✅ Scraper Function (Fetching Full Post Content)\n",
    "def scrape_reddit(batch_size=20):\n",
    "    \"\"\"Fetch high-intent lead posts from Reddit with full content in batches of 20.\"\"\"\n",
    "    leads = []\n",
    "    \n",
    "    for subreddit in SUBREDDITS:\n",
    "        try:\n",
    "            sub = reddit.subreddit(subreddit)\n",
    "            print(f\"🔍 Searching in r/{subreddit}...\")  \n",
    "\n",
    "            # ✅ Fetch from multiple sources (limiting to batch_size)\n",
    "            posts = list(sub.new(limit=batch_size)) + list(sub.hot(limit=batch_size // 2)) + list(sub.top(limit=batch_size // 2))\n",
    "\n",
    "            for post in posts:  \n",
    "                post_text = post.selftext if post.selftext else \"No content available\"\n",
    "                full_text = f\"{post.title} {post_text}\"  \n",
    "\n",
    "                # ✅ Extract email (if any) from title & full post\n",
    "                email_found = extract_email(full_text)\n",
    "\n",
    "                # ✅ DEBUG: Print Every Post Fetched\n",
    "                print(f\"\\n📌 Found Post: {post.title} | URL: {post.url} | Created: {datetime.utcfromtimestamp(post.created_utc)}\")\n",
    "                print(f\"📝 Full Content: {post_text[:500]}...\")  # Show first 500 characters\n",
    "\n",
    "                # ✅ Check if post contains relevant keywords (Fuzzy Matching)\n",
    "                if fuzzy_match(full_text, KEYWORDS) or fuzzy_match(post.title.lower(), KEYWORDS):\n",
    "                    print(f\"✅ MATCH FOUND: {post.title}\")  \n",
    "\n",
    "                    leads.append({\n",
    "                        \"Post ID\": post.id,\n",
    "                        \"Title\": post.title,\n",
    "                        \"Full Post\": post_text,  # ✅ Now stores full content\n",
    "                        \"Post URL\": post.url,\n",
    "                        \"Posted On\": datetime.utcfromtimestamp(post.created_utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"Comments\": post.num_comments,\n",
    "                        \"Email Found\": email_found  # ✅ Email extracted and stored\n",
    "                    })\n",
    "\n",
    "                time.sleep(1.5)  # ✅ Avoid Rate Limiting\n",
    "\n",
    "                # ✅ Stop when batch size is reached\n",
    "                if len(leads) >= batch_size:\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch data from r/{subreddit}: {e}\")\n",
    "\n",
    "        # ✅ Stop if batch size is reached\n",
    "        if len(leads) >= batch_size:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(leads)\n",
    "\n",
    "# ✅ Run the scraper (Batch of 20 records)\n",
    "reddit_leads_df = scrape_reddit(batch_size=20)\n",
    "\n",
    "# ✅ Display results in Jupyter Lab\n",
    "display(reddit_leads_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894eac4c-138e-4f58-83b3-3e7ea6acf499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
